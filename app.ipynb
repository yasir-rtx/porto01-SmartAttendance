{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587b1bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ba2a67e2f3475684b305ef60ef34a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppress TensorFlow warnings and info messages\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENCV_LOG_LEVEL\"]=\"SILENT\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Available backend options are: \"jax\", \"torch\", \"tensorflow\".\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Camera 1...\n",
      "Camera 1 is not Available!\n",
      "Opening Camera 2...\n",
      "Camera 2 is not Available!\n",
      "Opening Default Camera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@4.107] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@4.206] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@4.206] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n",
      "[ERROR:0@4.206] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749668361.606962  199019 service.cc:145] XLA service 0x71e5c806bc60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749668361.606990  199019 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 Ti, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749668365.253799  199019 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model preloaded successfully.\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.096102338878568\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.048056192013911\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.660873099049292\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.059120958427028\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 14.391417289351471\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.999649620591342\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 14.547447046720213\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 14.154198464231621\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 14.140044559356639\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 15.049685926522411\n",
      "\n",
      "Detected 1 face(s).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Identifing faces...\n",
      "Identity: Yasir, Distance: 14.881258010583593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preload the model to boostup the performance\n",
    "preload()\n",
    "\n",
    "# Access camera\n",
    "cap = getCamera()\n",
    "\n",
    "# Check if the video capture has been initialized correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video device.\")\n",
    "    raise SystemExit\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    # Check if the frame was read correctly\n",
    "    if not ret:\n",
    "        print(\"Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB format\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Flip the frame horizontally (optional)\n",
    "    frame = cv2.flip(frame, 1)  # Flip horizontally for a mirror effect\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    # The detectFace function returns the coordinates of detected faces/*\n",
    "    # If no faces are detected, it returns None\n",
    "    faces = detectFace(frame)\n",
    "\n",
    "    if faces is not None:\n",
    "        # Loop through the detected faces\n",
    "        for x, y, width, height in faces:\n",
    "            # Draw rectangles around detected face\n",
    "            frame = drawRectangle(x, y, width, height, frame)\n",
    "\n",
    "            # Extract the face region from the frame\n",
    "            face = frame[y:y+height, x:x+width]\n",
    "            \n",
    "            # Extract embeddings from the detected face\n",
    "            embeddedFaces = extractFaceEmbeddings(face, frame)\n",
    "\n",
    "            # Save the face embeddings to a file\n",
    "            # saveFaceEmbeddings(embeddedFaces)\n",
    "            \n",
    "            # Face Recognition Processing\n",
    "            faceRecognition(x=x, y=y, width=width, height=height, mindistance=20, frame=frame, embeddedFaces=embeddedFaces)\n",
    "\n",
    "    # Convert the frame to BGR format\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Display the frame in a window\n",
    "    cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "    # Wait for 1 ms and check if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "# Note: The video capture object is released and all OpenCV windows are closed\n",
    "# when the loop is exited, either by pressing 'q' or if an error occurs.\n",
    "# This ensures that resources are cleaned up properly.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
